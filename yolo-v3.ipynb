{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pycocotools\n!pip install wandb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\n!git clone https://github.com/KaushalJadhav/config_for_YOLOV3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\n!rm -r config_for_YOLOV3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import division\nimport pycocotools\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn as nn\nimport cv2\nfrom collections import defaultdict\nimport json\nimport tempfile\nimport matplotlib.pyplot as plt\nimport argparse\nimport yaml\nimport random\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:30:42.499624Z","iopub.execute_input":"2021-12-17T15:30:42.500385Z","iopub.status.idle":"2021-12-17T15:30:45.259763Z","shell.execute_reply.started":"2021-12-17T15:30:42.500329Z","shell.execute_reply":"2021-12-17T15:30:45.258714Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def preprocess(img, imgsize, jitter, random_placing=False):\n    h, w, _ = img.shape\n    img = img[:, :, ::-1]\n    assert img is not None\n\n    if jitter > 0:\n        # add jitter\n        # resize the images to random scale. Uniform distribution used.\n        dw = jitter * w\n        dh = jitter * h\n        new_ar = (w + np.random.uniform(low=-dw, high=dw))/(h + np.random.uniform(low=-dh, high=dh))\n    else:\n        new_ar = w / h\n\n    if new_ar < 1:             # need to do this to avoid rounding errors in typecasting to integer \n        nh = imgsize\n        nw = nh * new_ar\n    else:\n        nw = imgsize\n        nh = nw / new_ar\n    nw, nh = int(nw), int(nh)\n\n    if random_placing:   # \n        dx = int(np.random.uniform(imgsize - nw))\n        dy = int(np.random.uniform(imgsize - nh))\n    else:\n        dx = (imgsize - nw) // 2\n        dy = (imgsize - nh) // 2\n\n    img = cv2.resize(img, (nw, nh))\n    sized = np.ones((imgsize, imgsize, 3), dtype=np.uint8) * 127\n    sized[dy:dy+nh, dx:dx+nw, :] = img\n\n    info_img = (h, w, nh, nw, dx, dy)\n    return sized, info_img\n\ndef rand_scale(s):\n    scale = np.random.uniform(low=1, high=s)\n    if np.random.rand() > 0.5:\n        return scale\n    return 1 / scale\n\ndef random_distort(img, hue, saturation, exposure):\n    dhue = np.random.uniform(low=-hue, high=hue)\n    dsat = rand_scale(saturation)\n    dexp = rand_scale(exposure)\n\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n    img = np.asarray(img, dtype=np.float32) / 255.\n    img[:, :, 1] *= dsat\n    img[:, :, 2] *= dexp\n    H = img[:, :, 0] + dhue\n\n    if dhue > 0:\n        H[H > 1.0] -= 1.0\n    else:\n        H[H < 0.0] += 1.0\n\n    img[:, :, 0] = H\n    img = (img * 255).clip(0, 255).astype(np.uint8)\n    img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n    img = np.asarray(img, dtype=np.float32)\n\n    return img\n","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:30:45.262382Z","iopub.execute_input":"2021-12-17T15:30:45.262707Z","iopub.status.idle":"2021-12-17T15:30:45.281124Z","shell.execute_reply.started":"2021-12-17T15:30:45.262660Z","shell.execute_reply":"2021-12-17T15:30:45.280047Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class conv_labels():\n    def __init__(self,info_img,label2box,labels=None,box=None,maxsize=None,lrflip=None):\n        self.info_img=info_img\n        self.labels=labels\n        self.box=box\n        if label2box:\n            self.output=self.label2yolobox(maxsize,lrflip)\n        else:\n            self.output=self.yolobox2label()\n    def label2yolobox(self,maxsize,lrflip):\n        h, w, nh, nw, dx, dy = self.info_img\n        x1,y1 = self.labels[:, 1]/w,self.labels[:, 2] / h\n        x2,y2 = x1+ (self.labels[:, 3]/ w),y1+(self.labels[:, 4]/h)\n        self.labels[:, 1] = (((x1 + x2) / 2) * nw + dx) / maxsize\n        self.labels[:, 2] = (((y1 + y2) / 2) * nh + dy) / maxsize\n        self.labels[:, 3] *= nw / w / maxsize\n        self.labels[:, 4] *= nh / h / maxsize\n        if lrflip:\n            self.labels[:, 1] = 1 - self.labels[:, 1]\n        return self.labels\n\n\n    def yolobox2label(self):\n        h, w, nh, nw, dx, dy = self.info_img\n        y1, x1, y2, x2 = self.box\n        box_h = ((y2 - y1) / nh) * h\n        box_w = ((x2 - x1) / nw) * w\n        y1 = ((y1 - dy) / nh) * h\n        x1 = ((x1 - dx) / nw) * w\n        label = [y1, x1, y1 + box_h, x1 + box_w]\n        return label","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:30:45.282680Z","iopub.execute_input":"2021-12-17T15:30:45.283337Z","iopub.status.idle":"2021-12-17T15:30:45.299866Z","shell.execute_reply.started":"2021-12-17T15:30:45.283289Z","shell.execute_reply":"2021-12-17T15:30:45.298907Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class COCODataset(Dataset):\n    def __init__(self,model,data_dir,json_file='instances_train2017.json',train=True,img_size=416,augmentation=None,min_size=1,debug=False):\n        self.data_dir = data_dir\n        self.json_file = json_file\n        self.model = model\n        \n        self.coco = COCO(self.data_dir+'/annotations_trainval2017/annotations/'+self.json_file)\n        self.ids = self.coco.getImgIds()\n        if debug:\n            self.ids = self.ids[1:2]\n            print(\"debug mode...\", self.ids)\n        self.class_ids = sorted(self.coco.getCatIds())\n        self.name = 'train2017' if train else 'val2017'\n        self.max_labels = 50\n        self.img_size = img_size\n        self.min_size = min_size\n        if augmentation is not None:\n            self.lrflip = augmentation['LRFLIP']\n            self.jitter = augmentation['JITTER']\n            self.random_placing = augmentation['RANDOM_PLACING']\n            self.hue = augmentation['HUE']\n            self.saturation = augmentation['SATURATION']\n            self.exposure = augmentation['EXPOSURE']\n            self.random_distort = augmentation['RANDOM_DISTORT']\n\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, index):\n        id = self.ids[index]\n        annotations = self.get_annotations(id)\n        lrflip=self.get_lrflip()\n\n        # load image and preprocess\n        img=self.get_img(id)\n        assert img is not None\n        img,info_img=self.aug_img(img,lrflip)\n        # load labels\n        padded_labels=self.load_labels_with_padding(annotations,info_img,lrflip)\n\n        return img,padded_labels,info_img,id\n    \n    def get_annotations(self,id):\n        anno_ids = self.coco.getAnnIds(imgIds=[int(id)],iscrowd=None)\n        return self.coco.loadAnns(anno_ids)\n    \n    def get_lrflip(self):\n        if np.random.rand() > 0.5 and self.lrflip == True:\n            return True\n        return False\n    \n    def get_img(self,id):\n        img_file = os.path.join(self.data_dir, self.name,self.name,'{:012}'.format(id) + '.jpg')\n        img = cv2.imread(img_file)\n        if self.json_file == 'instances_val5k.json' and img is None:\n            img_file = os.path.join(self.data_dir, 'train2017','train2017','{:012}'.format(id) + '.jpg')\n            img = cv2.imread(img_file)\n        return img\n    \n    def aug_img(self,img,lrflip):\n        img,info_img =preprocess(img,self.img_size, jitter=self.jitter,random_placing=self.random_placing)\n        if self.random_distort:\n            img = random_distort(img,self.hue, self.saturation, self.exposure)\n        img = np.transpose(img/255.,(2, 0, 1))\n        if lrflip:\n            img = np.flip(img, axis=2).copy()\n        return img,info_img\n    \n    def load_labels_with_padding(self,annotations,info_img,lrflip):\n        labels = []\n        for anno in annotations:\n            if anno['bbox'][2] > self.min_size and anno['bbox'][3] > self.min_size:\n                labels.append([])\n                labels[-1].append(self.class_ids.index(anno['category_id']))\n                labels[-1].extend(anno['bbox'])\n        return self.get_padded_labels(labels,info_img,lrflip)\n    \n    def get_padded_labels(self,labels,info_img,lrflip):\n        padded_labels = np.zeros((self.max_labels, 5))\n        if len(labels) > 0:\n            labels = np.stack(labels)\n            if 'YOLO' in self.model:\n                conv= conv_labels(info_img,True,labels=labels,maxsize=self.img_size,lrflip=lrflip)\n                labels = conv.output\n            padded_labels[range(len(labels))[:self.max_labels]] = labels[:self.max_labels]\n        padded_labels = torch.from_numpy(padded_labels)\n        return padded_labels","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:31:09.044055Z","iopub.execute_input":"2021-12-17T15:31:09.044341Z","iopub.status.idle":"2021-12-17T15:31:09.070078Z","shell.execute_reply.started":"2021-12-17T15:31:09.044309Z","shell.execute_reply":"2021-12-17T15:31:09.067812Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class YOLOLayer(nn.Module):\n    def __init__(self,config_model,layer_no,in_ch,threshold=0.7):\n\n        super(YOLOLayer, self).__init__()\n        self.strides = [32,16,8] # fixed\n        self.anchors = config_model['ANCHORS']\n        self.layer_no=layer_no\n        self.anchor_mask = config_model['ANCH_MASK'][self.layer_no]\n        self.n_anchors = len(self.anchor_mask)\n        self.n_classes = config_model['N_CLASSES']\n        self.threshold= threshold\n        self.loss={\n            \"L2\":nn.MSELoss(size_average=False),\n            \"BCE\":nn.BCELoss(size_average=False)\n        }\n        self.anchors_grid = [(w /self.strides[self.layer_no],h/self.strides[self.layer_no]) for w, h in self.anchors] # Downsampling anchors\n        self.masked_anchors = [self.anchors_grid[i] for i in self.anchor_mask]\n        self.ref_anchors = np.zeros((len(self.anchors_grid), 4))\n        self.ref_anchors[:, 2:] = np.array(self.anchors_grid)\n        self.ref_anchors = torch.FloatTensor(self.ref_anchors)\n        self.conv = nn.Conv2d(in_channels=in_ch,out_channels=self.n_anchors*(self.n_classes + 5),kernel_size=1,stride=1,padding=0)\n\n    def forward(self,x,labels=None):\n        n_ch = 5+self.n_classes   # added 5 for the box co-ordinates\n        self.dtype = torch.cuda.FloatTensor if x.is_cuda else torch.FloatTensor\n        \n        output = self.get_output(x,n_ch)\n        # calculate pred - xywh obj cls\n\n        x_shift = self.dtype(np.broadcast_to(np.arange(self.fsize, dtype=np.float32), output.shape[:4]))\n        y_shift = self.dtype(np.broadcast_to(np.arange(self.fsize, dtype=np.float32).reshape(self.fsize, 1), output.shape[:4]))\n\n        pred=self.prediction(output)\n\n        if labels is None:  # not training\n            pred[..., :4] *= self.strides[self.layer_no]\n            return pred.contiguous().view(self.batchsize, -1, n_ch).data\n\n        pred = pred[..., :4].data\n        \n        # target assignment\n        target,obj_mask,tgt_mask,tgt_scale=self.initialise()\n        labels = labels.cpu().data\n        nlabel = (labels.sum(dim=2) > 0).sum(dim=1)  # number of objects\n        # Get the ground truth labels\n        truth_all=labels*self.fsize\n        truth_x_all = truth_all[:, :, 1]\n        truth_y_all = truth_all[:, :, 2]\n        truth_w_all = truth_all[:, :, 3]\n        truth_h_all = truth_all[:, :, 4]\n\n        for b in range(self.batchsize):\n            n = int(nlabel[b])\n            if n == 0:\n                continue\n            truth_box = self.dtype(np.zeros((n, 4)))\n            truth_box[:n, 2] = truth_w_all[b, :n]\n            truth_box[:n, 3] = truth_h_all[b, :n]\n            truth_i = truth_x_all.to(torch.int16).numpy()[b, :n]\n            truth_j = truth_y_all.to(torch.int16).numpy()[b, :n]\n\n            # calculate iou between truth and reference anchors\n            best_n,best_n_mask=self.best_n(truth_box)\n            \n            truth_box[:n, 0] = truth_x_all[b, :n]\n            truth_box[:n, 1] = truth_y_all[b, :n]\n            \n            pred_iou_best=self.pred_iou_best(pred[b],truth_box)\n            obj_mask[b]=~pred_iou_best\n\n            if sum(best_n_mask) == 0:\n                continue\n\n            for ti in range(best_n.shape[0]):\n                if best_n_mask[ti] == 1:\n                    i,j = truth_i[ti],truth_j[ti]\n                    a = best_n[ti]\n                    obj_mask[b, a, j, i] = 1\n                    tgt_mask[b, a, j, i, :] = 1\n                    target[b, a, j, i, 0] = self.roundint16(truth_x_all[b, ti])\n                    target[b, a, j, i, 1] = self.roundint16(truth_y_all[b, ti])\n                    target[b, a, j, i, 2] = self.log(truth_w_all,best_n,b,ti,0)\n                    target[b, a, j, i, 3] = self.log(truth_h_all,best_n,b,ti,1)\n                    target[b, a, j, i, 4] = 1\n                    target[b, a, j, i, 5 + labels[b, ti,0].to(torch.int16).numpy()] = 1\n                    tgt_scale[b, a, j, i, :] = torch.sqrt(2 - truth_w_all[b, ti] * truth_h_all[b, ti] /self.fsize /self.fsize)\n\n        # loss calculation\n        return self.get_losses(output,target,obj_mask,tgt_scale,tgt_mask,n_ch)\n    \n    def get_losses(self,output,target,obj_mask,tgt_scale,tgt_mask,n_ch):\n        y_out= output[..., 4]*obj_mask\n        n_out= output[..., 4]*(1-obj_mask)\n        output[..., np.r_[0:4, 5:n_ch]] *= tgt_mask\n        output[..., 2:4] *= tgt_scale\n        y_tar=target[..., 4]*obj_mask\n        n_tar=target[..., 4]*(1-obj_mask)\n        target[..., np.r_[0:4, 5:n_ch]] *= tgt_mask\n        target[..., 2:4] *= tgt_scale\n        \n        bceloss = nn.BCELoss(weight=tgt_scale*tgt_scale,size_average=False)  # weighted BCEloss\n        loss_xy = 5*self.loss[\"L2\"](output[..., :2], target[..., :2])\n        loss_wh = 5*self.loss[\"L2\"](output[..., 2:4], target[..., 2:4])\n        loss_obj = self.loss[\"BCE\"](y_out,y_tar)\n        loss_nobj = 0.5*self.loss[\"BCE\"](n_out,n_tar)\n        loss_cls = self.loss[\"BCE\"](output[..., 5:], target[..., 5:])\n        loss_l2 = self.loss[\"L2\"](output, target)\n        loss_coord=loss_xy + loss_wh\n        loss_iou=loss_obj+loss_nobj\n        loss = loss_coord + loss_iou + loss_cls\n        return loss,loss_coord,loss_iou,loss_cls,loss_l2\n    \n    def get_output(self,x,n_ch):\n        \"\"\"\n        Args:\n            x: input feature map  \n            n_ch: = number of co-ordinates used to specify bounding boxes = 5+self.n_classes\n        Returns:\n            output \n        \"\"\"\n        output = self.conv(x)\n        self.batchsize = output.shape[0]\n        self.fsize = output.shape[2]\n        output = output.view(self.batchsize, self.n_anchors,n_ch, self.fsize, self.fsize)\n        output = output.permute(0, 1, 3, 4, 2)  # .contiguous()\n            \n        # logistic activation for xy, obj, cls\n        output[..., np.r_[:2, 4:n_ch]] = torch.sigmoid(output[..., np.r_[:2, 4:n_ch]])\n        return output\n        \n    def prediction(self,output):\n        \n        masked_anchors = np.array(self.masked_anchors)\n        w_anchors = self.dtype(np.broadcast_to(np.reshape(masked_anchors[:, 0],(1, self.n_anchors, 1, 1)),output.shape[:4]))\n        h_anchors = self.dtype(np.broadcast_to(np.reshape(masked_anchors[:, 1],(1, self.n_anchors, 1, 1)),output.shape[:4]))\n        \n        # calculate pred - xywh obj cls\n        x_shift = self.dtype(np.broadcast_to(np.arange(self.fsize, dtype=np.float32), output.shape[:4]))\n        y_shift = self.dtype(np.broadcast_to(np.arange(self.fsize, dtype=np.float32).reshape(self.fsize, 1), output.shape[:4]))\n        \n        # calculate the offset values \n        pred = output.clone()\n        pred[..., 0] += x_shift\n        pred[..., 1] += y_shift\n        pred[..., 2] = torch.exp(pred[..., 2]) * w_anchors\n        pred[..., 3] = torch.exp(pred[..., 3]) * h_anchors\n        return pred\n    \n    def initialise(self):\n        obj_mask = torch.ones(self.batchsize, self.n_anchors,self.fsize, self.fsize).type(self.dtype)\n        tgt_mask = torch.zeros(self.batchsize, self.n_anchors,self.fsize, self.fsize, 4 + self.n_classes).type(self.dtype)\n        target = torch.zeros(self.batchsize,self.n_anchors,self.fsize,self.fsize,5+self.n_classes).type(self.dtype)\n        tgt_scale = torch.zeros(self.batchsize, self.n_anchors,self.fsize, self.fsize, 2).type(self.dtype)\n        return target,obj_mask,tgt_mask,tgt_scale\n    \n    def bboxes_iou(self,bboxes_a, bboxes_b, xyxy=True):\n        \"\"\"\n        Args:\n            bbox_a : An array whose shape is =(N, 4).\n                     where N is the number of bounding boxes.\n                     The dtype should be numpy.float32.\n            bbox_b : An array similar to bbox_a whose shape is (K, 4).\n                     The dtype should be :obj:`numpy.float32`.\n       Returns:\n           array: An array whose shape is (N, K).An element at index (n, k) contains IoUs between \n           nth bounding box in bbox_a and kth bounding box in bbox_b.\n       Reference: https://github.com/chainer/chainercv\n       \"\"\"\n        if bboxes_a.shape[1]!= 4 or bboxes_b.shape[1]!= 4:\n            raise IndexError\n\n        if xyxy:\n            # top left\n            tl = torch.max(bboxes_a[:, None, :2], bboxes_b[:, :2])\n            # bottom right\n            br = torch.min(bboxes_a[:, None, 2:], bboxes_b[:, 2:])\n            area_a = torch.prod(bboxes_a[:, 2:] - bboxes_a[:, :2], 1)\n            area_b = torch.prod(bboxes_b[:, 2:] - bboxes_b[:, :2], 1)\n        else:\n            # top left\n            tl = torch.max((bboxes_a[:, None, :2] - bboxes_a[:, None, 2:] / 2),(bboxes_b[:, :2] - bboxes_b[:, 2:] / 2))\n            # bottom right\n            br = torch.min((bboxes_a[:, None, :2] + bboxes_a[:, None, 2:] / 2),(bboxes_b[:, :2] + bboxes_b[:, 2:] / 2))\n            area_a = torch.prod(bboxes_a[:, 2:], 1)\n            area_b = torch.prod(bboxes_b[:, 2:], 1)\n        en = (tl < br).type(tl.type()).prod(dim=2)\n        area_i = torch.prod(br - tl, 2) * en  # * ((tl < br).all())\n        return area_i / (area_a[:, None] + area_b - area_i)\n    \n    def pred_iou_best(self,pred,truth_box):\n        \"\"\"\n        Args:\n            pred: prediction \n            truth_box: ground truth bounding box\n        Returns:\n            best iou among the iou calculated considering given pred and given ground truth bounding boxes\n        \"\"\"\n        pred_ious = self.bboxes_iou(pred.contiguous().view(-1, 4),truth_box,xyxy=False)\n        \n        # get the best iou\n        pred_iou_best, _ = pred_ious.max(dim=1)\n        pred_iou_best = (pred_iou_best > self.threshold) # check whether iou is greter than threshold\n        pred_iou_best = pred_iou_best.view(pred.shape[:3])\n        return pred_iou_best\n    \n    def best_n(self,truth_box):\n        anchor_ious_all = self.bboxes_iou(truth_box.cpu(),self.ref_anchors)\n        best_n_all=np.argmax(anchor_ious_all,axis=1)\n        best_n=best_n_all % 3\n        # Choose the best anchor box. Note that we can use OR operation here to optimize code. \n        best_n_mask = ((best_n_all == self.anchor_mask[0]) | (best_n_all == self.anchor_mask[1]) | (best_n_all == self.anchor_mask[2]))\n        return best_n,best_n_mask \n    \n    def log(self,num,den,b,ti,temp=0):\n        return torch.log(num[b,ti] / torch.Tensor(self.masked_anchors)[den[ti],temp] + 1e-16)\n    \n    def roundint16(self,arr):\n        return arr-arr.to(torch.int16).to(torch.float)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_conv(in_ch:int,out_ch:int,kernel_size,stride):\n        \"\"\"\n        Add a conv2d,batchnorm and leaky ReLU block.\n        Args:\n             in_ch : number of input channels of the convolution layer.\n             out_ch : number of output channels of the convolution layer.\n             kernel_size: kernel size of the convolution layer.\n             stride : stride of the convolution layer.\n        Returns:\n             stage: Sequential layers composing a convolution block.\n        \"\"\"\n        stage = nn.Sequential()\n        pad = (kernel_size-1)//2   # Zero padding\n        stage.add_module('conv', nn.Conv2d(in_channels=in_ch,out_channels=out_ch,kernel_size=kernel_size,stride=stride,padding=pad,bias=False))\n        stage.add_module('batch_norm',nn.BatchNorm2d(out_ch))\n        stage.add_module('leaky',nn.LeakyReLU(0.1))\n        return stage\n    \nclass resblock(nn.Module):\n    \"\"\"\n    Sequential residual blocks\n    Args:\n        ch: number of input and output channels. (Number of input and output channels is equal)\n        nblocks: number of residual blocks. Default=1\n        shortcut: if True, residual addition is enabled else disabled. Default=True\n    \"\"\"\n    def __init__(self, ch:int, nblocks=1, shortcut=True):\n        \n        super().__init__()  # Inheritance\n        self.shortcut = shortcut\n        self.module_list = nn.ModuleList()\n        for i in range(nblocks):\n            '''\n            Each residual block  contains 2 convolutional layers. \n            1. nn.Conv2d with input channels=ch,output channels=ch//2,kernel_size=1,stride=1,padding=0\n            2. nn.Conv2d with input channels=ch//2,output channels=ch,kernel_size=3,stride=1,padding=1\n            '''\n            resblock = nn.ModuleList()\n            resblock.append(add_conv(ch, ch//2, 1, 1))\n            resblock.append(add_conv(ch//2, ch, 3, 1))\n            self.module_list.append(resblock)   # Appends the residual block\n\n    def forward(self, x):\n        for module in self.module_list:\n            y=x\n            for res in module:\n                y=res(y)\n            x = x + y if self.shortcut else y\n        return x\n\n\n\n\nclass YOLOv3(nn.Module):\n    \"\"\"\n    YOLOv3 model module. The module list is defined by create_yolov3_modules function.The network returns \n    loss values from three YOLO layers during training and detection results during test.\n    \"\"\"\n    def __init__(self,config_model,threshold=0.7):\n        \"\"\"\n        Initialization of YOLOv3 class.\n        Args:\n            config_model (dict): used in YOLOLayer.\n            threshold(float): used in YOLOLayer.\n        \"\"\"\n        super(YOLOv3, self).__init__()\n        \n        self.config_model=config_model\n        self.threshold=threshold\n\n        if config_model['TYPE'] == 'YOLOv3':\n            self.module_list = nn.ModuleList()\n            self.create_yolov3_modules()\n        else:\n            raise Exception('Model name {} is not available'.format(config_model['TYPE']))\n\n    def forward(self, x, targets=None):\n        \"\"\"\n        Forward path of YOLOv3.\n        Args:\n            x: input data \n            targets: label array`\n        Returns:\n                output: output array\n        \"\"\"\n        train = True if targets is not None else False\n        output = []\n        self.loss_dict = defaultdict(float)\n        route_layers = []\n        for i,module in enumerate(self.module_list):\n            # yolo layers\n            if i in [14, 22, 28]:\n                if train:\n                    x,*loss_dict = module(x, targets)\n                    for name, loss in zip(['coord', 'iou','cls', 'l2'],loss_dict):\n                        self.loss_dict[name] += loss\n                else:\n                    x = module(x)   \n                output.append(x)\n            else:\n                x = module(x)\n\n            # route layers\n            \n            if i in [6,8,12,20]:\n                route_layers.append(x)\n            if i == 14:\n                x = route_layers[2]    # Realising shortcut connection\n            if i == 22:  # yolo 2nd\n                x = route_layers[3]\n            if i == 16:\n                x = torch.cat((x,route_layers[1]),1)\n            if i == 24:\n                x = torch.cat((x,route_layers[0]),1)\n        if train:\n            return sum(output)\n        else:\n            return torch.cat(output,1)\n    \n    def create_yolov3_modules(self):\n        \"\"\"\n        Build yolov3 layer modules.\n        Args:\n            config_model: model configuration.\n            threshold: used in YOLOLayer.\n        Returns:\n            module_list: YOLOv3 module list.\n        \"\"\"\n\n       # DarkNet53\n       # Reference- Table-1 of paper- YOLOv3: An Incremental Improvement\n    \n        self.module_list.append(add_conv(in_ch=3,out_ch=32,kernel_size=3,stride=1))      \n        self.module_list.append(add_conv(in_ch=32, out_ch=64,kernel_size=3,stride=2)) \n    \n        self.module_list.append(resblock(ch=64))   # Contains only one residual block \n    \n        self.module_list.append(add_conv(in_ch=64,out_ch=128,kernel_size=3,stride=2))\n    \n        self.module_list.append(resblock(ch=128,nblocks=2))\n    \n        self.module_list.append(add_conv(in_ch=128,out_ch=256,kernel_size=3,stride=2))\n    \n        self.module_list.append(resblock(ch=256,nblocks=8))   \n    \n        self.module_list.append(add_conv(in_ch=256,out_ch=512,kernel_size=3,stride=2))\n    \n        self.module_list.append(resblock(ch=512,nblocks=8))   \n    \n        self.module_list.append(add_conv(in_ch=512, out_ch=1024, kernel_size=3, stride=2))\n        self.module_list.append(resblock(ch=1024, nblocks=4))\n\n        # YOLOv3\n        self.module_list.append(resblock(ch=1024, nblocks=2, shortcut=False))\n        self.module_list.append(add_conv(in_ch=1024, out_ch=512,kernel_size=1,stride=1))\n    \n        # 1st yolo branch\n        self.module_list.append(add_conv(in_ch=512,out_ch=1024,kernel_size=3,stride=1))\n        self.module_list.append(YOLOLayer(self.config_model,layer_no=0,in_ch=1024,threshold=self.threshold))\n        self.module_list.append(add_conv(in_ch=512, out_ch=256, kernel_size=1,stride=1))\n        self.module_list.append(nn.Upsample(scale_factor=2,mode='nearest'))\n        self.module_list.append(add_conv(in_ch=768, out_ch=256, kernel_size=1,stride=1))\n        self.module_list.append(add_conv(in_ch=256, out_ch=512, kernel_size=3,stride=1))\n        self.module_list.append(resblock(ch=512, nblocks=1, shortcut=False))\n        self.module_list.append(add_conv(in_ch=512, out_ch=256, kernel_size=1,stride=1))\n    \n        # 2nd yolo branch\n        self.module_list.append(add_conv(in_ch=256, out_ch=512, kernel_size=3, stride=1))\n        self.module_list.append(YOLOLayer(self.config_model, layer_no=1, in_ch=512, threshold=self.threshold))\n\n        self.module_list.append(add_conv(in_ch=256, out_ch=128, kernel_size=1, stride=1))\n        self.module_list.append(nn.Upsample(scale_factor=2,mode='nearest'))\n        self.module_list.append(add_conv(in_ch=384, out_ch=128, kernel_size=1, stride=1))\n        self.module_list.append(add_conv(in_ch=128, out_ch=256, kernel_size=3, stride=1))\n        self.module_list.append(resblock(ch=256, nblocks=2, shortcut=False))\n        self.module_list.append(YOLOLayer(self.config_model,layer_no=2,in_ch=256,threshold=self.threshold))","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:31:19.485584Z","iopub.execute_input":"2021-12-17T15:31:19.486068Z","iopub.status.idle":"2021-12-17T15:31:19.547131Z","shell.execute_reply.started":"2021-12-17T15:31:19.486018Z","shell.execute_reply":"2021-12-17T15:31:19.546185Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"class COCOAPIEvaluator():\n    \"\"\"\n    COCO AP Evaluation class.\n    All the data in the val2017 dataset are processed and evaluated by COCO API.\n    \"\"\"\n    def __init__(self,model,data_dir,img_size,conf_thresh,nms_thresh=0.45):\n        \"\"\"\n        Args:\n            model: model name specified in config file\n            data_dir: dataset root directory\n            img_size: image size after preprocess. images are resized to squares whose shape is (img_size, img_size).\n            conf_thresh: confidence threshold ranging from 0 to 1,which is defined in the config file.\n            nms_thresh: IoU threshold of non-max supression ranging from 0 to 1.\n        \"\"\"\n\n        augmentation = {'LRFLIP': False, 'JITTER': 0, 'RANDOM_PLACING': False,\n                        'HUE': 0, 'SATURATION': 0, 'EXPOSURE': 0, 'RANDOM_DISTORT': False}\n\n        self.dataset = COCODataset(model=model,data_dir=data_dir,img_size=img_size,augmentation=augmentation,\n                                   json_file='instances_val2017.json',train=False)\n        self.dataloader = torch.utils.data.DataLoader(self.dataset, batch_size=1, shuffle=False, num_workers=0)\n        self.img_size = img_size\n        self.conf_thresh = 0.005 # from darknet\n        self.nms_thresh = nms_thresh # 0.45 (darknet)\n        self.num_classes=80\n    \n    def get_dtype(self):\n        cuda = torch.cuda.is_available()\n        dtype= torch.cuda.FloatTensor if cuda else torch.FloatTensor\n        return dtype\n\n    def evaluate(self, model):\n        \"\"\"\n        COCO average precision (AP) Evaluation. Iterate inference on the test dataset\n        and the results are evaluated by COCO API.\n        Args:\n            model : model object\n        Returns:\n            ap50_95 (float) : calculated COCO AP for IoU=50:95\n            ap50 (float) : calculated COCO AP for IoU=50\n        \"\"\"\n        model.eval()\n        dtype=self.get_dtype()\n        ids = []\n        data_dict = []\n        dataiterator = iter(self.dataloader)\n        while True: # all the data in val2017\n            try:\n                img, _, info_img, id_ = next(dataiterator)  # load a batch\n            except StopIteration:\n                break\n            info_img = [float(info) for info in info_img]\n            id_ = int(id_)\n            ids.append(id_)\n            with torch.no_grad():\n                img = Variable(img.type(dtype))\n                outputs = model(img)\n                outputs = self.postprocess(outputs)\n                if outputs[0] is None:\n                    continue\n                outputs = outputs[0].cpu().data\n\n            for output in outputs:\n                x1,y1,x2,y2 = float(output[:4])\n                label = self.dataset.class_ids[int(output[6])]\n                conv=conv_labels(info_img,label2box=False,box=(y1, x1, y2, x2))\n                box = conv.output\n                bbox = [box[1], box[0], box[3] - box[1], box[2] - box[0]]\n                score = float(output[4].data.item() * output[5].data.item()) # object score * class score\n                A = {\"image_id\": id_, \"category_id\": label, \"bbox\": bbox,\n                     \"score\": score, \"segmentation\": []} # COCO json format\n                data_dict.append(A)\n\n        return self.eval(data_dict)\n    \n    def eval(self,data_dict):\n        annType = ['segm', 'bbox', 'keypoints']\n        # Evaluate the Dt (detection) json comparing with the ground truth\n        if len(data_dict) > 0:\n            cocoGt = self.dataset.coco\n            # workaround: temporarily write data to json file because pycocotools can't process dict in py36.\n            _, tmp = tempfile.mkstemp()\n            json.dump(data_dict, open(tmp, 'w'))\n            cocoDt = cocoGt.loadRes(tmp)\n            cocoEval = COCOeval(self.dataset.coco, cocoDt, annType[1])\n            cocoEval.params.imgIds = ids\n            cocoEval.evaluate()\n            cocoEval.accumulate()\n            cocoEval.summarize()\n            return cocoEval.stats[0], cocoEval.stats[1]\n        else:\n            return 0, 0\n        \n    def postprocess(self,pred):\n        box_corner = pred.new(pred.shape)\n        box_corner[:, :, 0] = pred[:, :, 0] - pred[:, :, 2] / 2\n        box_corner[:, :, 1] = pred[:, :, 1] - pred[:, :, 3] / 2\n        box_corner[:, :, 2] = pred[:, :, 0] + pred[:, :, 2] / 2\n        box_corner[:, :, 3] = pred[:, :, 1] + pred[:, :, 3] / 2\n        pred[:, :, :4] = box_corner[:, :, :4]\n\n        output = [None for _ in range(len(pred))]\n        for i, img_pred in enumerate(pred):\n             # Filter out confidence scores below threshold\n            img_pred=self.filter_pred(img_pred)\n            # If none are remaining => process next image\n            if not img_pred.size(0):\n                continue\n        # Get detections with higher confidence scores than the threshold\n        detections=self.get_detections(img_pred)\n        \n        # Iterate through all predicted classes\n        unique_labels =self.get_unique_labels( detections,pred)\n        for c in unique_labels:\n            # Get the detections with the particular class\n            detections_class = self.get_detection_class(detections,c)\n            if output[i] is None:\n                output[i] = detections_class\n            else:\n                output[i] = torch.cat((output[i], detections_class))\n\n        return output\n    \n    def filter_pred(self,img_pred):\n        class_pred = torch.max(img_pred[:, 5:5 + self.num_classes], 1)\n        class_pred = class_pred[0]\n        conf_mask = (img_pred[:, 4] * class_pred >= self.conf_thresh).squeeze()\n        img_pred = img_pred[conf_mask]\n        return img_pred\n    \n    def nms(self,bbox,score=None,limit=None):\n        \"\"\"Suppress bounding boxes according to their IoUs and confidence scores.\n        Args:\n             bbox: Bounding boxes to be transformed.\n             thresh: Threshold of IoUs.\n             score: An array of confidences.\n             limit: The upper bound of the number of the output bounding boxes. \n                    If it is not specified, this method selects as many bounding boxes as possible.\n        Returns:\n             array: An array with indices of bounding boxes that are selected.\n                    They are sorted by the scores of bounding boxes in descending order.\n        \"\"\"\n\n        if len(bbox) == 0:\n            return np.zeros((0,),dtype=np.int32)\n\n        if score is not None:\n            order = score.argsort()[::-1]\n            bbox = bbox[order]   # reorder bboxes\n        \n        bbox_area = np.prod(bbox[:, 2:] - bbox[:, :2], axis=1) \n        selec = np.zeros(bbox.shape[0], dtype=bool)\n        for i,b in enumerate(bbox):\n            tl = np.maximum(b[:2],bbox[selec,:2])\n            br = np.minimum(b[2:],bbox[selec,2:])\n            area = np.prod(br - tl, axis=1) * (tl < br).all(axis=1)\n            iou = area / (bbox_area[i] + bbox_area[selec] - area)\n            if (iou >= self.nms_thresh).any():\n                continue\n            selec[i] = True\n            if limit is not None and np.count_nonzero(selec) >= limit:\n                break\n\n        selec = np.where(selec)[0]\n        if score is not None:\n            selec = order[selec]\n        return selec.astype(np.int32)\n   \n    def get_detections(self,img_pred):\n        ind = (img_pred[:, 5:] * img_pred[:, 4][:, None] >= self.conf_thresh).nonzero()\n        # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n        detections = torch.cat((img_pred[ind[:, 0], :5],img_pred[ind[:, 0], 5 + ind[:, 1]].unsqueeze(1),ind[:, 1].float().unsqueeze(1)),1)\n        return detections\n    \n    def get_unique_labels(self,detections,pred):\n        unique_labels = detections[:, -1].cpu().unique()\n        if pred.is_cuda:\n            unique_labels = unique_labels.cuda()\n        return unique_labels\n    \n    def get_detection_class(self,detections,c):\n        detections_class = detections[detections[:, -1] == c]\n        nms_in = detections_class.cpu().numpy()\n        nms_out_index = self.nms(nms_in[:, :4],score=nms_in[:, 4]*nms_in[:, 5])\n        detections_class = detections_class[nms_out_index]\n        return detections_class","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:31:29.412854Z","iopub.execute_input":"2021-12-17T15:31:29.413360Z","iopub.status.idle":"2021-12-17T15:31:29.457268Z","shell.execute_reply.started":"2021-12-17T15:31:29.413322Z","shell.execute_reply":"2021-12-17T15:31:29.455989Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"checkpoint_dir=\"/kaggle/working/checkpoints\"\ncfg_path= \"/kaggle/working/config_for_YOLOV3/yolov3_default.cfg\"\nweights_path=None\ncheckpoint= None\ndebug=False\ndatadir=\"/kaggle/input/coco2017\"\ncuda=True\neval_interval=100\ncheckpoint_interval=10","metadata":{"execution":{"iopub.status.busy":"2021-12-17T15:31:35.820062Z","iopub.execute_input":"2021-12-17T15:31:35.820402Z","iopub.status.idle":"2021-12-17T15:31:35.828416Z","shell.execute_reply.started":"2021-12-17T15:31:35.820368Z","shell.execute_reply":"2021-12-17T15:31:35.826836Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"wandb.login()\ncuda = torch.cuda.is_available()\nos.makedirs(checkpoint_dir,exist_ok=True)\n# Parse config settings\nwith open(cfg_path, 'r') as f:\n    cfg = yaml.safe_load(f)\nprint(\"successfully loaded config file: \", cfg)\nmomentum = cfg['TRAIN']['MOMENTUM']\ndecay = cfg['TRAIN']['DECAY']\nburn_in = cfg['TRAIN']['BURN_IN']\niter_size = cfg['TRAIN']['MAXITER']\nsteps = eval(cfg['TRAIN']['STEPS'])\nbatch_size = cfg['TRAIN']['BATCHSIZE']\nsubdivision = cfg['TRAIN']['SUBDIVISION']\nignore_thre = cfg['TRAIN']['IGNORETHRE']\nrandom_resize = cfg['AUGMENTATION']['RANDRESIZE']\nbase_lr = cfg['TRAIN']['LR'] / batch_size / subdivision\nwandb.init(name=\"YOLO-V3\",config=cfg,project=\"YOLO-V3\",resume='allow',id=\"samplerun4\")\nprint('effective_batch_size = batch_size * iter_size = %d * %d' %(batch_size, subdivision))\n\n# Learning rate setup\ndef burnin_schedule(i):\n    if i < burn_in:\n        factor = pow(i / burn_in, 4)\n    elif i < steps[0]:\n        factor = 1.0\n    elif i < steps[1]:\n        factor = 0.1\n    else:\n        factor = 0.01\n    return factor\n\n# Initiate model\nmodel = YOLOv3(cfg['MODEL'],threshold=ignore_thre)\nif checkpoint is not None:\n    print(\"loading pytorch ckpt...\",checkpoint)\n    state = torch.load(checkpoint)\n    if 'model_state_dict' in state.keys():\n        model.load_state_dict(state['model_state_dict'])\n    else:\n        model.load_state_dict(state)\n\nif cuda:\n    print(\"using cuda\") \n    model = model.cuda()\n\nmodel.train()\n\nimgsize = cfg['TRAIN']['IMGSIZE']\ndataset = COCODataset(model=cfg['MODEL']['TYPE'],data_dir=datadir,img_size=imgsize,augmentation=cfg['AUGMENTATION'],debug=False)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\ndataiterator = iter(dataloader)\n\nevaluator = COCOAPIEvaluator(model=cfg['MODEL']['TYPE'],data_dir=datadir,img_size=cfg['TEST']['IMGSIZE'],\n                             conf_thresh=cfg['TEST']['CONFTHRE'],nms_thresh=cfg['TEST']['NMSTHRE'])\n\ndtype = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n\n# optimizer setup\n# set weight decay only on conv.weight\nparams_dict = dict(model.named_parameters())\nparams = []\nfor key, value in params_dict.items():\n    if 'conv.weight' in key:\n        params += [{'params':value, 'weight_decay':decay * batch_size * subdivision}]\n    else:\n        params += [{'params':value, 'weight_decay':0.0}]\noptimizer = optim.SGD(params, lr=base_lr, momentum=momentum,dampening=0, weight_decay=decay * batch_size * subdivision)\niter_state = 0\nif checkpoint is not None:\n    if 'optimizer_state_dict' in state.keys():\n        optimizer.load_state_dict(state['optimizer_state_dict'])\n        iter_state = state['iter'] + 1\nscheduler = optim.lr_scheduler.LambdaLR(optimizer, burnin_schedule)\n\n# start training loop\nfor iter_i in range(iter_state, iter_size + 1):\n\n    # COCO evaluation\n    if iter_i % eval_interval == 0 and iter_i > 0:\n        ap50_95, ap50 = evaluator.evaluate(model)\n        model.train()\n        print('val/COCOAP50', ap50, iter_i)\n        print('val/COCOAP50_95', ap50_95, iter_i)\n        wandb.log(\n            {'val/COCOAP50': ap50,\n             'val/COCOAP50_95': ap50_95,\n             'val_step':iter_i/eval_interval\n            }\n        )\n\n    # subdivision loop\n    optimizer.zero_grad()\n    total_loss=[]\n    for inner_iter_i in range(subdivision):\n        try:\n            imgs, targets, _, _ = next(dataiterator)  # load a batch\n        except StopIteration:\n            dataiterator = iter(dataloader)\n            imgs, targets, _, _ = next(dataiterator)  # load a batch\n        imgs = Variable(imgs.type(dtype))\n        targets = Variable(targets.type(dtype), requires_grad=False)\n        loss = model(imgs, targets)\n        wandb.log(\n            {'Train_loss/Step':float(loss),\n            'train_step':iter_i+inner_iter_i}\n        )\n        loss.backward()\n        total_loss.append(float(loss))\n        \n    optimizer.step()\n    scheduler.step()\n\n    # logging\n    current_lr = scheduler.get_last_lr()[0] * batch_size * subdivision\n    print('[Iter %d/%d] [lr %f][Losses: coord %f, iou %f,cls %f, l2 %f, imgsize %d]'\n          % (iter_i, iter_size, current_lr,model.loss_dict['coord'],model.loss_dict['iou'], model.loss_dict['cls'], \n             model.loss_dict['l2'], imgsize),flush=True)\n    wandb.log(\n        {\n            'Learning Rate': current_lr,\n            'Total Loss/Epoch': sum(total_loss)/len(total_loss),\n            'train_epoch':iter_i}\n    )\n    # random resizing\n    if random_resize:\n            imgsize = (random.randint(0, 9) % 10 + 10) * 32\n            dataset.img_shape = (imgsize, imgsize)\n            dataset.img_size = imgsize\n            dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n            dataiterator = iter(dataloader)\n\n    # save checkpoint\n    if iter_i > 0 and (iter_i % checkpoint_interval == 0):\n        torch.save({'iter': iter_i,'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict(),},\n                   os.path.join(checkpoint_dir, \"YOLOV3-iter()\"+str(iter_i)+\".ckpt\"))\n        wandb.save(os.path.join(checkpoint_dir, \"YOLOV3-iter()\"+str(iter_i)+\".ckpt\"))\n        \nwandb.finish()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}